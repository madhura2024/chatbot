# -*- coding: utf-8 -*-
"""chatbot.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Y0OmSm7Pyy1pDn1nikTX5WcKgOPXQwyj
"""

# duolingo_tutor_ollama_full.py
# Tutor: Dataset + Ollama every time (both responses shown)

import os

import random
import json
import sqlite3
import subprocess
from datetime import datetime
import pandas as pd
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sentence_transformers import SentenceTransformer
from datasets import load_dataset

# ==========================
# CONFIG
# ==========================
OLLAMA_MODEL = os.environ.get("OLLAMA_MODEL", "tinyllama")  # changed default to tinyllama
MASTER_THRESHOLD = 3
NEW_PROBABILITY = 0.7
TOP_K = 3
DB_PATH = "tutor_state.db"

# ==========================
# SQLITE STATE
# ==========================
def init_db():
    conn = sqlite3.connect(DB_PATH)
    cur = conn.cursor()
    cur.execute("""
    CREATE TABLE IF NOT EXISTS lessons (
        id INTEGER PRIMARY KEY,
        type TEXT,
        key TEXT UNIQUE,
        payload TEXT,
        correct_streak INTEGER DEFAULT 0,
        attempts INTEGER DEFAULT 0,
        last_seen TEXT,
        mastered INTEGER DEFAULT 0
    );
    """)
    conn.commit()
    return conn

conn = init_db()

# ==========================
# INTENT DATASET
# ==========================
data = {
    "text": [
        "hello", "hi", "hey", "good morning", "good evening", "good afternoon", "howdy", "greetings", "what's up",
        "teach me a new word", "give me a vocabulary word", "word of the day", "i want to learn vocabulary",
        "teach me idioms", "show me new words", "vocabulary practice", "i want to memorize words",
        "explain past tense", "how do you use present perfect", "what is future tense", "teach me grammar",
        "explain present continuous", "what is conditional tense", "difference between say and tell", "when to use been or gone",
        "translate apple to spanish", "how do you say cat in french", "what is dog in german",
        "translate hello to japanese", "how do you say thank you in italian",
        "teach me some basic phrases", "what are common greetings", "give me travel phrases",
        "conversation practice", "how to say polite phrases", "daily phrases learning",
        "give me a quiz", "test my english", "ask me a grammar question", "quiz me on vocabulary", "english practice test",
        "correct my sentence", "is this sentence correct", "fix my grammar", "correct this: he go to school",
        "check my sentence", "grammar correction needed",
        "i need help", "can you help me", "iâ€™m confused", "explain english rules", "help me with grammar"
    ],
    "label": (
        ["greeting"]*9
        + ["vocabulary"]*8
        + ["grammar"]*8
        + ["translation"]*5
        + ["phrases"]*6
        + ["quiz"]*5
        + ["correction"]*6
        + ["help"]*5
    )
}
df = pd.DataFrame(data)

# ==========================
# TF-IDF classifier
# ==========================
vectorizer = TfidfVectorizer(ngram_range=(1,2), stop_words="english")
X = vectorizer.fit_transform(df["text"])
y = df["label"]
model = LogisticRegression(max_iter=1000)
model.fit(X, y)

# ==========================
# Embedding model & KB
# ==========================
embedding_model = SentenceTransformer("all-MiniLM-L6-v2")
knowledge_chunks = []
knowledge_embeddings = []

def cosine_similarity(a, b):
    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))

def retrieve_kb(query, topk=3):
    if not knowledge_embeddings:
        return []
    q_emb = embedding_model.encode([query], show_progress_bar=False)[0]
    sims = [cosine_similarity(q_emb, k) for k in knowledge_embeddings]
    idxs = np.argsort(sims)[-topk:][::-1]
    return [knowledge_chunks[i] for i in idxs]

# ==========================
# LESSON SEEDING
# ==========================
def generate_vocab_lessons_from_generics(n=10):
    ds = load_dataset("generics_kb", "generics_kb_best", split="train")
    lessons, seen_keys = [], set()
    for example in ds.shuffle(seed=42):
        sentence = example.get("generic_sentence") or example.get("sentence")
        if not sentence:
            continue
        words = [w.strip(".,!?;:()\"'").lower() for w in sentence.split() if len(w) > 3]
        if not words:
            continue
        word = random.choice(words)
        key = f"word_{word}"
        if key in seen_keys:
            continue
        seen_keys.add(key)
        lessons.append({
            "type": "vocabulary",
            "key": key,
            "word": word,
            "definition": "Example usage from GenericsKB:",
            "translation": sentence
        })
        if len(lessons) >= n:
            break
    return lessons

def seed_lessons(conn, lessons):
    global knowledge_chunks, knowledge_embeddings
    cur = conn.cursor()
    for l in lessons:
        payload = json.dumps(l, ensure_ascii=False)
        try:
            cur.execute("INSERT INTO lessons (type, key, payload) VALUES (?, ?, ?)", (l["type"], l["key"], payload))
        except sqlite3.IntegrityError:
            pass
    conn.commit()
    knowledge_chunks = [l["translation"] for l in lessons if l["type"] == "vocabulary"]
    knowledge_embeddings = embedding_model.encode(knowledge_chunks, show_progress_bar=False)

seed_lessons(conn, generate_vocab_lessons_from_generics(20))

# ==========================
# LESSON HELPERS
# ==========================
def pick_activity(conn):
    cur = conn.cursor()
    cur.execute("SELECT COUNT(*) FROM lessons WHERE attempts=0")
    new_left = cur.fetchone()[0]
    cur.execute("SELECT COUNT(*) FROM lessons WHERE mastered=0 AND attempts>0")
    review_left = cur.fetchone()[0]
    if new_left > 0 and random.random() < NEW_PROBABILITY:
        return "new"
    elif review_left > 0:
        return "review"
    else:
        return "new"

def get_new_lesson(conn, lesson_type=None):
    cur = conn.cursor()
    if lesson_type:
        cur.execute("SELECT id, payload FROM lessons WHERE attempts=0 AND type=? LIMIT 1", (lesson_type,))
    else:
        cur.execute("SELECT id, payload FROM lessons WHERE attempts=0 LIMIT 1")
    r = cur.fetchone()
    return {"id": r[0], **json.loads(r[1])} if r else None

def get_review_lesson(conn, lesson_type=None):
    cur = conn.cursor()
    if lesson_type:
        cur.execute("SELECT id, payload FROM lessons WHERE mastered=0 AND attempts>0 AND type=? ORDER BY last_seen LIMIT 1", (lesson_type,))
    else:
        cur.execute("SELECT id, payload FROM lessons WHERE mastered=0 AND attempts>0 ORDER BY last_seen LIMIT 1")
    r = cur.fetchone()
    return {"id": r[0], **json.loads(r[1])} if r else None

def update_stats(conn, lesson_id, correct):
    cur = conn.cursor()
    cur.execute("SELECT correct_streak, attempts FROM lessons WHERE id=?", (lesson_id,))
    s, a = cur.fetchone()
    a += 1
    s = s + 1 if correct else 0
    mastered = 1 if s >= MASTER_THRESHOLD else 0
    cur.execute("UPDATE lessons SET correct_streak=?, attempts=?, last_seen=?, mastered=? WHERE id=?",
                (s, a, datetime.utcnow().isoformat(), mastered, lesson_id))
    conn.commit()

# ==========================
# RESPONSES
# ==========================
def template_response(intent, lesson, user_input):
    if intent == "greeting":
        return "Hello! ðŸ‘‹ Ready for a lesson or quiz?"
    if intent == "vocabulary" and lesson:
        return f"Word: {lesson['word']} â€” {lesson['definition']} Bengali: {lesson['translation']}"
    if intent == "grammar" and lesson:
        return f"Rule: {lesson['rule']} Example: {lesson['example']}"
    if intent == "phrases" and lesson:
        return f"Phrase: {lesson['phrase']} â€” {lesson['usage']} Bengali: {lesson['translation']}"
    if intent == "idioms" and lesson:
        return f"Idiom: {lesson['idiom']} = {lesson['meaning']}"
    if intent == "translation":
        info = retrieve_kb(user_input, topk=TOP_K)
        return "Relevant info:\n" + "\n".join(info)
    if intent == "quiz":
        return "Letâ€™s try a quiz!"
    if intent == "correction":
        return "Correction: 'He goes to school.' (3rd person needs -s)."
    if intent == "help":
        return "I can teach vocab, grammar, phrases, idioms, quizzes, corrections."
    return "Try 'teach me a word' or 'give me a quiz'."

def create_duolingo_prompt(user_input, lesson, user_progress):
    if not lesson:
        return f"User input: {user_input}\nUser progress: {user_progress}\nProvide a clear helpful response."
    if lesson["type"] == "vocabulary":
        return f"Teach '{lesson['word']}' in Bengali: '{lesson['translation']}'. Example and practice."
    elif lesson["type"] == "grammar":
        return f"Explain rule: {lesson['rule']}. Example: {lesson['example']}. Ask question."
    elif lesson["type"] == "phrases":
        return f"Teach phrase '{lesson['phrase']}' in Bengali: '{lesson['translation']}'. Ask practice."
    elif lesson["type"] == "idioms":
        return f"Explain idiom '{lesson['idiom']}' = '{lesson['meaning']}'. Ask example sentence."
    else:
        return f"User input: {user_input}\nProvide a helpful response."

OLLAMA_PATH = r"C:\Users\Madhura\AppData\Local\Programs\Ollama\ollama.exe"

def call_ollama(prompt, model=OLLAMA_MODEL, timeout=60):
    try:
        process = subprocess.Popen(
            [OLLAMA_PATH, "run", model],
            stdin=subprocess.PIPE,
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE
        )
        # Send prompt encoded in utf-8 and decode output/errors safely
        try:
            out_bytes, err_bytes = process.communicate(prompt.encode('utf-8'), timeout=timeout)
        except Exception as e:
            process.kill()
            process.wait()
            return f"[Ollama-ERR] {e}"

        out = out_bytes.decode('utf-8', errors='replace').strip() if out_bytes else ""
        err = err_bytes.decode('utf-8', errors='replace').strip() if err_bytes else ""

        if out:
            return out
        if err:
            return f"[Ollama-ERR] {err}"
        return None
    except Exception as e:
        return f"[Ollama-ERR] {e}"

def show_progress(conn):
    cur = conn.cursor()
    cur.execute("SELECT key, payload, mastered, attempts FROM lessons")
    rows = cur.fetchall()
    mastered, inprog, unseen = [], [], []
    for k, p, m, a in rows:
        d = json.loads(p)
        name = d.get("word") or d.get("phrase") or d.get("rule") or d.get("idiom")
        if m:
            mastered.append(name)
        elif a > 0:
            inprog.append(name)
        else:
            unseen.append(name)
    print(f"Bot: Progress ðŸ“Š\n- Mastered: {len(mastered)} {mastered}\n- In progress: {len(inprog)} {inprog}\n- Unseen: {len(unseen)} {unseen}")

# ==========================
# MAIN LOOP
# ==========================
user_progress = {lbl: {"correct": 0} for lbl in df["label"].unique()}

print(f"Using Ollama model: {OLLAMA_MODEL}\n")
print("Language Tutor (Dataset + Ollama) ready â€” type 'exit' to quit. Type 'progress' to see DB progress.\n")

while True:
    userinput = input("You: ").strip()
    if not userinput:
        continue
    if userinput.lower() in ["exit", "quit"]:
        print("Bot: Goodbye! ðŸ‘‹")
        break
    if userinput.lower() == "progress":
        show_progress(conn)
        continue

    # --- Intent detection (always dataset TF-IDF) ---
    uservec = vectorizer.transform([userinput])
    pred_label = model.predict(uservec)[0]

    # --- Pick activity & lesson ---
    activity = pick_activity(conn)
    lesson = None
    if pred_label in ["vocabulary","grammar","phrases","idioms"]:
        lesson = get_new_lesson(conn, pred_label) if activity == "new" else get_review_lesson(conn, pred_label)

    # --- Dataset reply ---
    ds_reply = template_response(pred_label, lesson, userinput)
    print("Bot (Dataset):", ds_reply)

    # --- Ollama reply ---
    prompt = create_duolingo_prompt(userinput, lesson, user_progress)
    ollama_reply = call_ollama(prompt)
    print("Bot (Ollama):", ollama_reply)
